{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c812a66-6a6a-490d-b36a-7f2f335a5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18 March Assignment Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14515d0-c115-4f99-a527-17f022815250",
   "metadata": {},
   "source": [
    "### Ans 1:\n",
    "The Filter method in feature selection is a technique used in machine learning to select the most relevant features from a dataset before training a model. This method relies on the statistical properties of the features themselves, independent of any machine learning algorithm. Here's a detailed explanation of how the Filter method works:\n",
    "\n",
    "### Key Concepts and Steps\n",
    "\n",
    "1. **Independent Evaluation**:\n",
    "   - The Filter method evaluates each feature individually based on certain statistical criteria.\n",
    "   - It does not involve any machine learning algorithm during the feature selection process, making it computationally efficient.\n",
    "\n",
    "2. **Criteria for Evaluation**:\n",
    "   - **Correlation Coefficient**: Measures the linear relationship between each feature and the target variable. Commonly used correlation measures include Pearson, Spearman, and Kendall tau.\n",
    "   - **Mutual Information**: Quantifies the amount of information obtained about one variable through another variable, capturing non-linear relationships as well.\n",
    "   - **Chi-Square Test**: Assesses whether there is a significant association between categorical features and the target variable.\n",
    "   - **Variance Threshold**: Eliminates features with low variance, assuming they have little predictive power.\n",
    "   - **ANOVA (Analysis of Variance)**: Compares the means of different groups and is typically used for numeric features against categorical target variables.\n",
    "\n",
    "3. **Process**:\n",
    "   - **Compute Scores**: Each feature is scored based on the chosen statistical measure.\n",
    "   - **Rank Features**: Features are ranked according to their scores.\n",
    "   - **Select Top Features**: A threshold is set (either a fixed number of features or a score cutoff), and only the top-ranking features are selected for model training.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Simplicity and Speed**: Filter methods are simple to implement and computationally efficient, making them suitable for high-dimensional datasets.\n",
    "- **Independence**: They are independent of any machine learning algorithms, ensuring that the feature selection process is not biased by a specific model's performance.\n",
    "- **Interpretability**: The criteria used (e.g., correlation, mutual information) are straightforward and interpretable.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- **Ignore Interactions**: Since each feature is evaluated independently, interactions between features are not considered.\n",
    "- **Potentially Suboptimal**: The selected features might not be the best combination for a particular model, as the method does not take model performance into account.\n",
    "\n",
    "### Example Workflow\n",
    "\n",
    "1. **Calculate Correlation**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from sklearn.datasets import load_boston\n",
    "\n",
    "   # Load dataset\n",
    "   data = load_boston()\n",
    "   X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "   y = pd.Series(data.target)\n",
    "\n",
    "   # Calculate Pearson correlation\n",
    "   correlations = X.apply(lambda x: x.corr(y))\n",
    "   ```\n",
    "\n",
    "2. **Rank Features**:\n",
    "   ```python\n",
    "   ranked_features = correlations.abs().sort_values(ascending=False)\n",
    "   ```\n",
    "\n",
    "3. **Select Top Features**:\n",
    "   ```python\n",
    "   top_features = ranked_features.head(5).index.tolist()\n",
    "   X_selected = X[top_features]\n",
    "   ```\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "- **Preprocessing**: As an initial step in the data preprocessing pipeline to reduce dimensionality.\n",
    "- **Exploratory Data Analysis (EDA)**: To identify and visualize the most significant features related to the target variable.\n",
    "- **Baseline Model**: As a baseline feature selection method before applying more complex techniques like Wrapper or Embedded methods.\n",
    "\n",
    "In summary, the Filter method is a fast and efficient way to perform feature selection based on statistical measures. While it may not always yield the best subset of features for a particular model, it is a valuable tool for initial feature reduction and gaining insights into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c6a74-405f-4d5b-b6b4-3823c6793056",
   "metadata": {},
   "source": [
    "### Ans 2:\n",
    "Key Differences\n",
    "Dependency on Model: The Wrapper method depends on a specific machine learning model for feature selection, while the Filter method is model-agnostic.\n",
    "\n",
    "Computation: The Wrapper method is computationally more intensive due to repeated model training and evaluation, whereas the Filter method is faster and more efficient.\n",
    "\n",
    "Feature Interactions: The Wrapper method can account for interactions between features, while the Filter method evaluates each feature independently.\n",
    "\n",
    "Outcome: The Wrapper method typically provides a feature subset that is optimized for the specific model and task, whereas the Filter method provides a generally good subset of features based on statistical relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44726846-3921-4859-9c2e-07576375089b",
   "metadata": {},
   "source": [
    "### Ans 3:\n",
    "\n",
    "Embedded feature selection methods integrate the feature selection process within the model training. These methods are often more efficient than Wrapper methods because they utilize the learning algorithm itself to determine which features are most important, often as part of the model training process. Here are some common techniques used in Embedded feature selection methods:\n",
    "\n",
    "### 1. Regularization Methods\n",
    "Regularization techniques add a penalty to the model for having large coefficients, which can effectively shrink some coefficients to zero, thus performing feature selection.\n",
    "\n",
    "- **Lasso (L1 Regularization)**: The Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty equal to the absolute value of the magnitude of coefficients. This can force some coefficients to be exactly zero, effectively performing feature selection.\n",
    "  ```python\n",
    "  from sklearn.linear_model import Lasso\n",
    "\n",
    "  model = Lasso(alpha=0.1)  # alpha is the regularization strength\n",
    "  model.fit(X, y)\n",
    "  selected_features = X.columns[model.coef_ != 0]\n",
    "  ```\n",
    "\n",
    "- **Ridge (L2 Regularization)**: Ridge regression adds a penalty equal to the square of the magnitude of coefficients. Although it does not perform feature selection (since it does not shrink coefficients to zero), it is often used alongside Lasso in the Elastic Net method.\n",
    "  \n",
    "- **Elastic Net**: This method combines both L1 and L2 regularization, providing a balance between the two.\n",
    "  ```python\n",
    "  from sklearn.linear_model import ElasticNet\n",
    "\n",
    "  model = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio balances L1 and L2 regularization\n",
    "  model.fit(X, y)\n",
    "  selected_features = X.columns[model.coef_ != 0]\n",
    "  ```\n",
    "\n",
    "### 2. Decision Tree-Based Methods\n",
    "Decision tree algorithms and their ensembles (like Random Forests and Gradient Boosting) inherently perform feature selection by evaluating the importance of each feature in making splits in the tree.\n",
    "\n",
    "- **Decision Trees**: Features that are used more frequently in decision splits are considered more important.\n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "  model = DecisionTreeClassifier()\n",
    "  model.fit(X, y)\n",
    "  feature_importances = model.feature_importances_\n",
    "  selected_features = X.columns[feature_importances > threshold]\n",
    "  ```\n",
    "\n",
    "- **Random Forests**: An ensemble of decision trees that averages the feature importance scores across all trees.\n",
    "  ```python\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "  model = RandomForestClassifier()\n",
    "  model.fit(X, y)\n",
    "  feature_importances = model.feature_importances_\n",
    "  selected_features = X.columns[feature_importances > threshold]\n",
    "  ```\n",
    "\n",
    "- **Gradient Boosting Machines (GBM)**: Similar to Random Forests, but builds trees sequentially, with each tree trying to correct the errors of the previous one.\n",
    "  ```python\n",
    "  from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "  model = GradientBoostingClassifier()\n",
    "  model.fit(X, y)\n",
    "  feature_importances = model.feature_importances_\n",
    "  selected_features = X.columns[feature_importances > threshold]\n",
    "  ```\n",
    "\n",
    "### 3. Regularized Linear Models with Embedded Feature Selection\n",
    "Some linear models have built-in feature selection capabilities due to regularization techniques:\n",
    "\n",
    "- **Logistic Regression with L1 Regularization**: Similar to Lasso, but for classification problems.\n",
    "  ```python\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "  model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "  model.fit(X, y)\n",
    "  selected_features = X.columns[model.coef_[0] != 0]\n",
    "  ```\n",
    "\n",
    "### 4. Feature Importance from Other Models\n",
    "Other models can also provide feature importance scores as part of their output:\n",
    "\n",
    "- **Support Vector Machines (SVM) with L1 Penalty**: Like Lasso, can perform feature selection by shrinking some weights to zero.\n",
    "  ```python\n",
    "  from sklearn.svm import LinearSVC\n",
    "\n",
    "  model = LinearSVC(penalty='l1', dual=False)\n",
    "  model.fit(X, y)\n",
    "  selected_features = X.columns[model.coef_[0] != 0]\n",
    "  ```\n",
    "\n",
    "### 5. Tree-Based Feature Selection Methods\n",
    "Some specific methods are designed to interpret the results of tree-based models to select features:\n",
    "\n",
    "- **Boruta**: A wrapper method around Random Forests designed to find all relevant features, not just the top features.\n",
    "  ```python\n",
    "  from boruta import BorutaPy\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "  model = RandomForestClassifier()\n",
    "  boruta = BorutaPy(model, n_estimators='auto', random_state=42)\n",
    "  boruta.fit(X.values, y.values)\n",
    "  selected_features = X.columns[boruta.support_]\n",
    "  ```\n",
    "\n",
    "### Summary\n",
    "Embedded feature selection methods leverage the model's training process to perform feature selection, often resulting in more efficient and effective identification of relevant features. Common techniques include regularization methods like Lasso and Elastic Net, decision tree-based methods, and models that inherently provide feature importance scores. These methods can often outperform simple Filter methods by considering feature interactions and the specific needs of the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab184d-84d1-463b-8607-37506b103086",
   "metadata": {},
   "source": [
    "### ANS 4:\n",
    "### Disadvantages\n",
    "\n",
    "- **Ignore Interactions**: Since each feature is evaluated independently, interactions between features are not considered.\n",
    "- **Potentially Suboptimal**: The selected features might not be the best combination for a particular model, as the method does not take model performance into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1015857-b69e-401b-86c8-73d360a846d9",
   "metadata": {},
   "source": [
    "### ANS 5:\n",
    "There are several situations where you might prefer using the Filter method over the Wrapper method for feature selection. Here are some key scenarios:\n",
    "\n",
    "### 1. **Large Datasets with High Dimensionality**\n",
    "- **Efficiency**: The Filter method is computationally efficient because it evaluates features independently of the model training process. This makes it suitable for datasets with a large number of features and instances where running multiple iterations of model training (as required by Wrapper methods) would be computationally expensive and time-consuming.\n",
    "- **Scalability**: For high-dimensional datasets, the Filter method can quickly reduce the feature space, making subsequent modeling steps more manageable.\n",
    "\n",
    "### 2. **Preprocessing and Initial Feature Reduction**\n",
    "- **Initial Screening**: When you need to perform an initial reduction of the feature space before applying more sophisticated and computationally expensive methods, the Filter method serves as a good first step. It can help eliminate irrelevant or redundant features early in the process.\n",
    "- **Baseline Feature Selection**: Filter methods can provide a baseline feature set that can be refined using other methods later. This can be particularly useful in the early stages of exploratory data analysis (EDA).\n",
    "\n",
    "### 3. **Independence from Learning Algorithms**\n",
    "- **Algorithm-Agnostic**: The Filter method is independent of any specific learning algorithm. If you are unsure about which model to use or want to keep your feature selection process model-agnostic, the Filter method is a good choice.\n",
    "- **Consistency Across Models**: Since Filter methods are not tied to a particular model, the selected features are generally useful across different models, providing a versatile feature set.\n",
    "\n",
    "### 4. **Avoiding Overfitting**\n",
    "- **Less Risk of Overfitting**: Filter methods rely on statistical properties of the data rather than model performance. This reduces the risk of overfitting that might occur with Wrapper methods, especially in situations with small datasets or when cross-validation is not feasible.\n",
    "\n",
    "### 5. **Simplicity and Interpretability**\n",
    "- **Ease of Interpretation**: Filter methods are often easier to understand and interpret because they use straightforward statistical measures (e.g., correlation, mutual information). This can be beneficial when you need to explain the feature selection process to stakeholders or when interpretability is crucial.\n",
    "- **Transparency**: The criteria used by Filter methods (such as variance, correlation, or chi-square tests) are transparent and easy to justify.\n",
    "\n",
    "### 6. **Computational Constraints**\n",
    "- **Limited Resources**: If computational resources are limited, the Filter method is preferable due to its lower computational demands. Wrapper methods require multiple iterations of model training and validation, which can be resource-intensive.\n",
    "\n",
    "### Example Scenarios\n",
    "\n",
    "- **Text Mining and Natural Language Processing (NLP)**: When working with text data, you often deal with thousands or even millions of features (e.g., words or n-grams). The Filter method can quickly identify and remove irrelevant or low-frequency terms.\n",
    "- **Genomic Data Analysis**: In bioinformatics, genomic datasets can have tens of thousands of features (genes). Using Filter methods to reduce dimensionality before applying more complex models is a common practice.\n",
    "- **Real-Time Systems**: In applications where real-time processing is required, such as online recommendation systems or fraud detection, the Filter method's speed and efficiency are advantageous.\n",
    "\n",
    "### Summary\n",
    "While Wrapper methods can provide more tailored feature sets optimized for specific models, they are computationally expensive and risk overfitting. The Filter method is a simpler, faster, and more interpretable approach suitable for initial feature reduction, high-dimensional datasets, and situations with computational constraints. By leveraging statistical properties of the data, Filter methods provide a robust baseline feature selection that can be refined further if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430138ab-3c5f-42e1-963d-1518b0d859cc",
   "metadata": {},
   "source": [
    "### ANS 6:\n",
    "To select the most pertinent attributes for a predictive model using the Filter Method, follow these steps:\n",
    "\n",
    "### Step-by-Step Process for Feature Selection Using Filter Method:\n",
    "\n",
    "1. **Understand the Dataset**:\n",
    "   - Begin with a thorough understanding of the dataset and the business context.\n",
    "   - Identify the target variable (churn) and the feature set (all other attributes).\n",
    "\n",
    "2. **Preprocess the Data**:\n",
    "   - Handle missing values through imputation or removal.\n",
    "   - Convert categorical variables to numerical formats using encoding techniques such as One-Hot Encoding or Label Encoding.\n",
    "   - Normalize or standardize the features if necessary.\n",
    "\n",
    "3. **Univariate Statistical Tests**:\n",
    "   - Apply statistical tests to score the relationship between each feature and the target variable.\n",
    "   - For continuous features, use statistical tests such as Pearson’s correlation coefficient.\n",
    "   - For categorical features, use tests like the Chi-Squared test.\n",
    "\n",
    "4. **Correlation Analysis**:\n",
    "   - Calculate the correlation matrix to identify highly correlated features with the target variable.\n",
    "   - For continuous features, use Pearson’s correlation.\n",
    "   - For categorical features, use methods like ANOVA or mutual information.\n",
    "\n",
    "5. **Feature Ranking and Selection**:\n",
    "   - Rank the features based on the results from the statistical tests and correlation analysis.\n",
    "   - Select the top-ranked features based on a threshold or a predefined number of features.\n",
    "\n",
    "6. **Evaluate Multicollinearity**:\n",
    "   - Check for multicollinearity among the selected features using Variance Inflation Factor (VIF).\n",
    "   - Remove features with high multicollinearity (high VIF values).\n",
    "\n",
    "7. **Iterative Validation**:\n",
    "   - Validate the selected features by training a baseline model and evaluating its performance.\n",
    "   - Iteratively refine the feature set based on model performance and insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfda3d1-b931-4498-a47a-1f6fe81b6d6a",
   "metadata": {},
   "source": [
    "### ANS 7:\n",
    "The Embedded method for feature selection integrates the feature selection process within the model training process. It uses algorithms that have built-in mechanisms to identify the most relevant features during the model fitting process. Here’s how you can use the Embedded method to select the most relevant features for predicting the outcome of a soccer match:\n",
    "\n",
    "Feature Selection Using the Embedded Method\n",
    "\n",
    "### Example Implementation in Python\n",
    "\n",
    "Below is an example of using the Embedded method with a Random Forest classifier to select the most relevant features for predicting the outcome of a soccer match.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('soccer_matches.csv')\n",
    "\n",
    "# Preprocess data\n",
    "# Handle missing values\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=['match_outcome'])\n",
    "y = df['match_outcome']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select top features\n",
    "top_features = features_df['Feature'].head(10)\n",
    "\n",
    "# Create a new dataset with only the top features\n",
    "X_train_selected = X_train[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "# Train the model again with selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test_selected)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy with selected features: {accuracy:.2f}')\n",
    "print('Selected Features:')\n",
    "print(features_df.head(10))\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Missing values are handled by filling with the mean.\n",
    "   - Categorical variables are encoded using One-Hot Encoding.\n",
    "\n",
    "2. **Model Training**:\n",
    "   - A Random Forest classifier is trained on the full feature set.\n",
    "   - Feature importances are extracted from the trained model.\n",
    "\n",
    "3. **Feature Selection**:\n",
    "   - Features are ranked by their importance scores.\n",
    "   - The top features are selected based on their importance scores.\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - The model is re-trained using only the selected top features.\n",
    "   - The model's performance is evaluated on the test set using accuracy as the metric.\n",
    "\n",
    "This approach ensures that only the most relevant features are selected during the model training process, leveraging the embedded feature selection capabilities of the Random Forest classifier. This method can be similarly applied using other models like Lasso regression for linear models or Gradient Boosting Machines for tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f403e4-bbbb-4564-9968-b8428db6aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANs 8:\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('house_prices.csv')\n",
    "\n",
    "# Preprocess data\n",
    "# Handle missing values\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Apply Recursive Feature Elimination (RFE)\n",
    "# Select the number of features to retain (e.g., 5)\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "\n",
    "# Train the model with the selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test_selected)\n",
    "r2_score = model.score(X_test_selected, y_test)\n",
    "\n",
    "print(f'R^2 Score with selected features: {r2_score:.2f}')\n",
    "print('Selected Features:')\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183dc0af-66cc-4ad3-ace2-0d7db5eaf55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
