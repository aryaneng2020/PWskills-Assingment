{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d834686-b681-434a-bf9c-aa3b7c14371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16 March Assignment Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e14efc-9977-4df8-8412-b6be8a8496c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS 1:\n",
    "'''\n",
    "Overfitting: Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations \n",
    "in the data rather than the underlying patterns. The consequences of overfitting include poor generalization to unseen data, \n",
    "high variance, and inaccurate predictions.\n",
    "\n",
    "Underfitting: Underfitting occurs when a model is too simplistic to capture the underlying structure of the data, \n",
    "resulting in high bias and poor performance on both the training and test datasets.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ead2ef-2d36-4bf1-a052-f1d408cee472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANs 2:\n",
    "'''\n",
    "To reduce overfitting, several techniques can be employed:\n",
    "\n",
    "Regularization: Introducing penalties on large model parameters to prevent them from becoming too complex.\n",
    "\n",
    "Cross-validation: Splitting the data into multiple subsets for training and validation, allowing for better estimation of the model's performance.\n",
    "\n",
    "Early stopping: Monitoring the model's performance on a validation set and stopping training when performance starts to degrade.\n",
    "\n",
    "Feature selection/reduction: Removing irrelevant or redundant features from the dataset to reduce model complexity.\n",
    "\n",
    "Ensemble methods: Combining multiple models to reduce overfitting by capturing different aspects of the data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d68d1-17d4-4dbc-b9bb-1453794e4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANs 3:\n",
    "'''\n",
    "Underfitting occurs when a model is too simplistic to capture the underlying structure of the data. This can happen in scenarios where:\n",
    "\n",
    "The model is too simple or has too few parameters relative to the complexity of the data.\n",
    "The features used for training the model are not informative enough to capture the relationships present in the data.\n",
    "The chosen algorithm is not powerful enough to capture the patterns in the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b75665-2ef0-4314-98d1-a8cdd7490add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANs 4:\n",
    "'''\n",
    "The bias-variance tradeoff refers to the tradeoff between the bias and variance of a model. Bias measures how well the model fits \n",
    "the training data, while variance measures how much the model's predictions vary with changes in the training dataset.\n",
    "\n",
    "High bias models (underfitting) have low complexity and may fail to capture the underlying patterns in the data.\n",
    "High variance models (overfitting) have high complexity and may fit the training data too closely, capturing noise or random fluctuations.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759a50a-cb86-4056-9467-8203bd65509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANs 5:\n",
    "'''\n",
    "Common methods for detecting overfitting and underfitting include:\n",
    "\n",
    "Learning curves: Plotting the model's performance (e.g., training and validation error) against the size of the training dataset\n",
    "can reveal patterns indicative of overfitting or underfitting.\n",
    "\n",
    "Validation curves: Varying model complexity or hyperparameters and observing changes in performance on a validation set can help \n",
    "diagnose overfitting or underfitting.\n",
    "\n",
    "Cross-validation: Splitting the data into multiple folds and assessing model performance on each fold can provide insights into \n",
    "whether the model is overfitting or underfitting.\n",
    "\n",
    "Bias-variance analysis: Decomposing the model's error into bias and variance components can help diagnose whether the model \n",
    "is suffering from underfitting or overfitting.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284d0bd-d125-442d-b157-5dddd455ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANs 6:\n",
    "'''\n",
    "Bias: High bias models (underfitting) have limited capacity to capture the underlying patterns in the data. \n",
    "Examples include linear regression models with too few features or low polynomial degrees.\n",
    "\n",
    "Variance: High variance models (overfitting) are overly complex and fit the training data too closely, capturing noise or random fluctuations. \n",
    "Examples include decision trees with very deep branches or neural networks with many layers.\n",
    "\n",
    "High bias models tend to have low training and test performance, as they are too simplistic to capture the underlying patterns. \n",
    "High variance models may have high training performance but poor test performance due to overfitting to noise in the training data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6165f-6e4b-488f-ac56-ec1222f7e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANs 7:\n",
    "'''\n",
    "Regularization is a technique used to prevent overfitting by adding penalties on large model parameters. Common regularization techniques include:\n",
    "\n",
    "L1 regularization (Lasso): Adds a penalty proportional to the absolute value of the coefficients, encouraging sparsity and feature selection.\n",
    "\n",
    "L2 regularization (Ridge): Adds a penalty proportional to the square of the coefficients, encouraging smaller coefficients and reducing model \n",
    "complexity.\n",
    "\n",
    "ElasticNet regularization: Combines L1 and L2 penalties, providing a balance between sparsity and model complexity.\n",
    "\n",
    "Dropout: A technique commonly used in neural networks that randomly drops a fraction of neurons during training to prevent\n",
    "co-adaptation and improve generalization.\n",
    "\n",
    "These regularization techniques help control the complexity of the model and prevent overfitting by penalizing large parameter values, \n",
    "encouraging simpler models that generalize better to unseen data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a7869-791a-40e8-9242-ef4e4e6d132b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
